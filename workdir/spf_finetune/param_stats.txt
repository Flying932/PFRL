SPF Frozen-Rep Finetune Parameter Stats
freeze_rep=True
actor_trainable=False
adapter_actor_grad=False
actor_on_adapter_weight=0.0
alpha_log_requires_grad=True
overall: total=712249, trainable=47675, frozen=664574, ratio=0.066936

[rep] total=630908, trainable=0, frozen=630908, ratio=0.000000
rep.trainable_names:
  - (none)
rep.frozen_names:
  - rep.cls_token
  - rep.decoder_cls_token
  - rep.pos_embed_spatial
  - rep.pos_embed_temporal
  - rep.pos_embed_class
  - rep.mask_token
  - rep.decoder_pos_embed_spatial
  - rep.decoder_pos_embed_temporal
  - rep.decoder_pos_embed_class
  - rep.patch_embed.value_embedding.tokenConv.weight
  - rep.patch_embed.temporal_embedding.weekday_embed.emb.weight
  - rep.patch_embed.temporal_embedding.day_embed.emb.weight
  - rep.patch_embed.temporal_embedding.month_embed.emb.weight
  - rep.blocks.0.norm1.weight
  - rep.blocks.0.norm1.bias
  - rep.blocks.0.attn.q.weight
  - rep.blocks.0.attn.q.bias
  - rep.blocks.0.attn.k.weight
  - rep.blocks.0.attn.k.bias
  - rep.blocks.0.attn.v.weight
  - rep.blocks.0.attn.v.bias
  - rep.blocks.0.attn.proj.weight
  - rep.blocks.0.attn.proj.bias
  - rep.blocks.0.norm2.weight
  - rep.blocks.0.norm2.bias
  - rep.blocks.0.mlp.fc1.weight
  - rep.blocks.0.mlp.fc1.bias
  - rep.blocks.0.mlp.fc2.weight
  - rep.blocks.0.mlp.fc2.bias
  - rep.norm.weight
  - rep.norm.bias
  - rep.decoder_embed.weight
  - rep.decoder_embed.bias
  - rep.decoder_blocks.0.norm1.weight
  - rep.decoder_blocks.0.norm1.bias
  - rep.decoder_blocks.0.attn.q_linear.weight
  - rep.decoder_blocks.0.attn.q_linear.bias
  - rep.decoder_blocks.0.attn.k_linear.weight
  - rep.decoder_blocks.0.attn.k_linear.bias
  - rep.decoder_blocks.0.attn.v_linear.weight
  - rep.decoder_blocks.0.attn.v_linear.bias
  - rep.decoder_blocks.0.attn.proj.weight
  - rep.decoder_blocks.0.attn.proj.bias
  - rep.decoder_blocks.0.norm2.weight
  - rep.decoder_blocks.0.norm2.bias
  - rep.decoder_blocks.0.mlp.fc1.weight
  - rep.decoder_blocks.0.mlp.fc1.bias
  - rep.decoder_blocks.0.mlp.fc2.weight
  - rep.decoder_blocks.0.mlp.fc2.bias
  - rep.decoder_norm.weight
  - rep.decoder_norm.bias
  - rep.decoder_pred.weight
  - rep.decoder_pred.bias
  - rep.contrastive_head.0.weight
  - rep.contrastive_head.0.bias
  - rep.contrastive_head.2.weight
  - rep.contrastive_head.2.bias

[adapter] total=13488, trainable=13488, frozen=0, ratio=1.000000
adapter.trainable_names:
  - adapter.bottom_adapter.norm.weight
  - adapter.bottom_adapter.norm.bias
  - adapter.bottom_adapter.down.weight
  - adapter.bottom_adapter.down.bias
  - adapter.bottom_adapter.up.weight
  - adapter.bottom_adapter.up.bias
  - adapter.mid_adapter.norm.weight
  - adapter.mid_adapter.norm.bias
  - adapter.mid_adapter.down.weight
  - adapter.mid_adapter.down.bias
  - adapter.mid_adapter.up.weight
  - adapter.mid_adapter.up.bias
  - adapter.top_adapter.norm.weight
  - adapter.top_adapter.norm.bias
  - adapter.top_adapter.down.weight
  - adapter.top_adapter.down.bias
  - adapter.top_adapter.up.weight
  - adapter.top_adapter.up.bias
adapter.frozen_names:
  - (none)

[actor] total=33666, trainable=0, frozen=33666, ratio=0.000000
actor.trainable_names:
  - (none)
actor.frozen_names:
  - actor.cls_token
  - actor.blocks.0.fc1.weight
  - actor.blocks.0.fc1.bias
  - actor.blocks.0.fc2.weight
  - actor.blocks.0.fc2.bias
  - actor.norm.weight
  - actor.norm.bias
  - actor.decoder_pred.weight
  - actor.decoder_pred.bias

[critic] total=34186, trainable=34186, frozen=0, ratio=1.000000
critic.trainable_names:
  - critic.cls_token
  - critic.blocks.0.fc1.weight
  - critic.blocks.0.fc1.bias
  - critic.blocks.0.fc2.weight
  - critic.blocks.0.fc2.bias
  - critic.norm.weight
  - critic.norm.bias
  - critic.decoder_pred.weight
  - critic.decoder_pred.bias
critic.frozen_names:
  - (none)
